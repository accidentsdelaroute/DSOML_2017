{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In this class we will code one of the oldest (and most efficient!) optimization methods (Cauchy, Augustin (1847). Méthode générale pour la résolution des systèmes d'équations simultanées).\n",
    "\n",
    "**The intuition**: go in the direction of steepest descent\n",
    "\n",
    "<img style=\"margin-left:0\" width=\"300px\" src=\"https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg\" />\n",
    "\n",
    "**More formally**:\n",
    "\n",
    "We want to minimize a function $f: \\mathbb{R}^p \\to \\mathbb{R}$ which is differentiable. Then we construct a sequence $x^1, x^2 , \\ldots$ by the recusive formula\n",
    "\n",
    "$$x^{k+1} = x^k - \\gamma \\nabla f(x^k) \\quad$$\n",
    "\n",
    "where $\\gamma$ is the step-size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step size\n",
    "\n",
    "* How to choose the step size?\n",
    "\n",
    "The theory says that the optimum is given by $\\frac{1}{L}$, where $L$ is the Lipschitz constant of the gradient of $f$.\n",
    "\n",
    "## Gradient descent for least squares\n",
    "\n",
    "We will now code a gradient descent scheme. The first thing is to define what is the loss that we want to optimize. We will start with a least squares loss:\n",
    "\n",
    "$$f(x) = \\frac{1}{2}\\|b - Ax\\|^2$$\n",
    "for some given matrices $A$ and vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples, n_features = 10, 5\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "b = np.random.randn(n_samples)\n",
    "\n",
    "def f(x):\n",
    "    return 0.5 * np.sum((b - np.dot(A, x)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.75849683799143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

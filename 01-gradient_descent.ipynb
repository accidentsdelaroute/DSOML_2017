{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In this class we will code one of the oldest (and most efficient!) optimization methods (Cauchy, Augustin (1847). _Méthode générale pour la résolution des systèmes d'équations simultanées_).\n",
    "\n",
    "## The intuition\n",
    "\n",
    "go in the direction of steepest descent\n",
    "\n",
    "<img style=\"margin-left:0\" width=\"300px\" src=\"https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg\" />\n",
    "\n",
    "## More formally\n",
    "\n",
    "We want to minimize a function $f: \\mathbb{R}^p \\to \\mathbb{R}$ which is differentiable. Then we construct a sequence $x^1, x^2 , \\ldots$ by the recusive formula\n",
    "\n",
    "$$x^{k+1} = x^k - \\gamma \\nabla f(x^k) \\quad$$\n",
    "\n",
    "where $\\gamma$ is the step-size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step size\n",
    "\n",
    "* How to choose the step size?\n",
    "\n",
    "The theory says that the optimum is given by $\\frac{1}{L}$, where $L$ is the Lipschitz constant of the gradient of $f$.\n",
    "\n",
    "## Gradient descent for least squares\n",
    "\n",
    "We will now code a gradient descent scheme. The first thing is to define what is the loss that we want to optimize. We will start with a least squares loss:\n",
    "\n",
    "$$f(x) = \\frac{1}{2}\\|b - Ax\\|^2$$\n",
    "for some given matrices $A$ and vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples, n_features = 10, 5\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "w = np.random.randn(n_features)\n",
    "b = A.dot(w) + np.random.randn(n_samples)\n",
    "\n",
    "def func(x):\n",
    "    return 0.5 * np.sum((b - np.dot(A, x)) ** 2)\n",
    "\n",
    "def grad(x):\n",
    "    return - A.T.dot(b - np.dot(A, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func([0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implemented algorithm:\n",
    "\n",
    "max_iter = 100\n",
    "# Lipschitz constant\n",
    "L = np.linalg.norm(A.T.dot(A))\n",
    "step_size = 1. / L\n",
    "# initial guess\n",
    "xk = np.zeros(n_features)\n",
    "for i in range(max_iter):\n",
    "    xk = xk - step_size * grad(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize convergence\n",
    "\n",
    "Once we started the algorithm. How to know if its working properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports matplotlib, a plotting library\n",
    "%pylab inline\n",
    "\n",
    "cost_history = []\n",
    "grad_history = []\n",
    "xk = np.zeros(n_features)\n",
    "for i in range(max_iter):\n",
    "    xk = xk - step_size * grad(xk)\n",
    "    cost_history.append(func(xk)) # .. insert this line to keep track of iterates ..\n",
    "    grad_history.append(np.linalg.norm(grad(xk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "plt.plot(cost_history, lw=4)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(grad_history, lw=4)\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (20 min): \n",
    "\n",
    "Use the gradient descent algorithm to solve a logistic regression problem. The data is given below. You will need to:\n",
    "\n",
    " * Define the function\n",
    " * Define the gradient\n",
    " * Compute the step size\n",
    " * Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples, n_features = 10, 5\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "w = np.random.randn(n_features)\n",
    "b = A.dot(w) + np.random.randn(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

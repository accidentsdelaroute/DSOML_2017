{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "\n",
    "![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[16].png)\n",
    "\n",
    "\n",
    "Stochastic gradient descent (SGD) is an optimization method that can be applied to problems in which we seek to minimize the expected value of a :\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{argmin}_x \\frac{1}{n} \\sum_{i=1}^n f_i(x) \\quad,\n",
    "$$\n",
    "\n",
    "This is ideally suited for machine learning problems, where $n$ is the number of samples.\n",
    "\n",
    "## The algorithm\n",
    "\n",
    "The algorithm is similar to gradient descent, with the key exception that instead of using the _full gradient_ we use the gradient of a random sample. At each iteration we choose $1 \\leq i \\leq n$ uniformly at random and perform the update:\n",
    "\n",
    "\n",
    "$$\n",
    "  x^{k+1} = x^k - \\gamma^k \\nabla f_i(x^k) \\quad.\n",
    "$$\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Continuing with the least squares problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples, n_features = 10, 5\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "w = np.random.randn(n_features)\n",
    "b = A.dot(w) + np.random.randn(n_samples)\n",
    "\n",
    "def func(x):\n",
    "    return 0.5 * np.sum((b - np.dot(A, x)) ** 2)\n",
    "\n",
    "def grad(x):\n",
    "    return - A.T.dot(b - np.dot(A, x))\n",
    "\n",
    "def partial_grad(x, i):\n",
    "    return - A[i] * (b[i] - np.dot(A[i], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implemented algorithm:\n",
    "\n",
    "max_iter = 100\n",
    "# Lipschitz constant\n",
    "L = np.linalg.norm(A.T.dot(A))\n",
    "step_size = 1. / L\n",
    "# initial guess\n",
    "xk = np.zeros(n_features)\n",
    "for i in range(max_iter):\n",
    "    idx = np.random.randint(0, n_samples) \n",
    "    xk = xk - step_size * partial_grad(xk, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports matplotlib, a plotting library\n",
    "%pylab inline\n",
    "\n",
    "cost_history = []\n",
    "grad_history = []\n",
    "xk = np.zeros(n_features)\n",
    "for i in range(max_iter):\n",
    "    xk = xk - step_size * partial_grad(xk, idx)\n",
    "    cost_history.append(func(xk)) # .. insert this line to keep track of iterates ..\n",
    "    grad_history.append(np.linalg.norm(grad(xk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(grad_history, lw=4)\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "  * What is going on?\n",
    "  * Why is it not converging?\n",
    "  * How to solve this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
